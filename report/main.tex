\documentclass[sigconf]{acmart}

\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

\usepackage{dblfloatfix}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage[htt]{hyphenat}
\usepackage[normalem]{ulem}

\begin{document}

\title{Comparative Study of Conversational Search Engine Retrieval Pipelines}
\subtitle{Group ID: \#5}

% AUTHORS:
\author{Til Mohr}
\affiliation{}
\email{tv.mohr@stud.uis.no}

\author{Ishaac Ourahou}
\affiliation{}
\email{i.ourahou@stud.uis.no}

% DATE:
\date{\today}



\begin{abstract}
In our master's degree program, we delved into the realms of information retrieval and text mining. Our group project focuses on developing a system capable of searching for specific passages within a conversation. This entails considering the historical context of previous exchanges when interpreting the current user's query. The system reformulates the user's query to better align with the conversation's overarching topic and then seeks the most relevant passages from a vast collection of 8.8 million documents, specifically from the MS MARCO document collection. We initially create a baseline information retrieval pipeline, which serves as our foundational reference. We then explore advanced techniques and methods in the field of conversational search engines. Our report concludes with a comparative analysis between the baseline system and our advanced retrieval mechanisms, outlining the strengths and shortcomings of our approaches, and offering conclusions.
\end{abstract}

\keywords{information retrieval, conversational search engine, conversational search system}

%% Remove copyright footer
\settopmatter{printacmref=false}
\setcopyright{none}
\renewcommand\footnotetextcopyrightpermission[1]{}
\pagestyle{plain}
%% ------------------------

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}\label{sec:intro}
With the rise of conversational agents like Amazon Alexa and Apple Siri, the ability to accurately and relevantly respond to user requests has become increasingly important. These agents are designed to interact fluidly with users, and the essence of their functionality rests on providing precise information in their responses.

Our project centers on building a conversational search engine, a system that not only addresses the user's immediate question but also takes into account previous queries and the ongoing conversation's topic. This provides a more holistic approach to the user's information needs. Initially, we will construct a basic architecture, establishing our baseline system. This baseline will serve as a comparison point against more sophisticated techniques. It's required, however, that even this foundational system meets or exceeds the performance of the reference system set by our project guides.

Our research will utilize the MS MARCO dataset, a comprehensive collection of 8.8 million documents. Efficiently retrieving relevant documents from this vast pool necessitates an indexing process, which we will execute using the \texttt{pyterrier} API.

Section \ref{sec:problem} will articulate the specific problem we aim to address. In Section \ref{sec:related} we will explore literature relevant to our retrieval pipelines. We will then elaborate on the methodologies behind our retrieval pipelines in Sections \ref{sec:baseline}, \ref{sec:baseline+rm3}, \ref{sec:doc2query-method}, and \ref{sec:doc2query-method+rm3}. Our report will conclude in Section \ref{sec:results}, where we evaluate and compare the effectiveness of our systems, highlighting the strengths and potential areas of enhancement in our advanced approaches.

The codebase for our project can be found on GitHub\footnote{URL: \url{https://github.com/CodingTil/2023_24---IRTM---Group-Project}}.

\section{Problem Statement}\label{sec:problem}
This project focuses on developing a Conversational Search Engine (CSE). What distinguishes a CSE from a traditional search engine is its inherent context-aware nature. In a CSE, queries asked by a user within a session are semantically interconnected. Rather than treating each query in isolation, a CSE leverages information from both the previous queries and the system's responses to those queries to retrieve relevant passages. It must cater to the continuous thread of a dialogue, while also being agile enough to accommodate sudden shifts in conversation topics. For every user query, the goal is to generate a ranked listing of the top 1000 passages out of an expansive 8.8 million document collection.

A vital component that sets a CSE apart is its query rewriting mechanism. This feature reformulates the presented query to better reflect the ongoing conversation topic. Figure \ref{figure:global_pipeline} offers a schematic representation of how the various stages of a CSE retrieval pipeline interact. Analogous to traditional search engines, a CSE also requires prior indexing of the document collection, enabling computationally efficient retrieval and ranking processes.

\begin{figure}[h]
	\includegraphics[width=7cm]{pipeline.png}
	\caption{Overall system's pipeline architecture}
	\label{figure:global_pipeline}
\end{figure}


\section{Related Work}\label{sec:related}
In this Section, we delve into the research encompassing the realms of conversational search engines and the broader area of information retrieval. While certain highlighted studies do not directly cater to conversational search engines or explicit information retrieval, their techniques remain invaluable in various stages of the conversational retrieval process.

\subsection{Pseudo-Relevance Feedback by Query Expansion}\label{sec:prf}
Oftentimes, queries entered by users are too short and include too little hint for the search engine to retrieve all relevant documents \cite{vaidyanathan2015query}. To counter this, the user-generated query must be reformulated to better reflect the user's informational needs. One approach to this is known as query expansion using relevance feedback: Assume we have a set of documents relevant to the user-generated query. One can then reformulate the query such that the retrieval system retrieves documents similar to the set of relevant documents, i.e. documents that are probably also relevant to the user's query. This expansion is accomplished by selecting keywords from the set of relevant documents and appending those to the query. Different strategies for this exist \cite{vaidyanathan2015query}.

However, in a real setting, we do not have a set of relevant documents. Instead, we can generate a set of documents that are hopefully relevant to the user's query, and then follow the same process. Since the relevance of the generated set of documents is unknown, this approach is called pseudo-relevance feedback. In order to retrieve this set, we can use the original user entered query to retrieve only the top $p$ documents. If $p$ is very small, it is likely that all those documents are relevant to the query. Expanding the query using those documents will then likely result in a better retrieval of all relevant documents \cite{vaidyanathan2015query}.

One particular technique for this is known as \texttt{RM3} \cite{abdul2004umass}. \texttt{RM3} uses relevance-based language models \cite{lavrenko2017relevance} to estimate the relevance of a word based on the word's probability in the language models of the (pseudo-)relevant documents. This relevance of the word to the query is then interpolated with the original query language model to avoid the query drifting too far away from the original query. The resulting language model is then used to generate a new query. Overall, the \texttt{RM3} approach can be parameterized in the number of documents to use for the pseudo-relevance feedback, the number of terms to add to the query, and the interpolation parameter \cite{abdul2004umass,lavrenko2017relevance,chen2022pseudo}.

\subsection{Text-to-Text Transfer Transformer}\label{sec:t5}
The vast domain of natural language processing (NLP) revolves around the understanding of natural language, whether presented in text or speech form. NLP aspires to equip computers with the capability to grasp the depth of human language and harness this understanding to execute a range of tasks, such as text summarization, machine translation, and question answering. Given the diverse nature of these tasks in terms of their input, output, and underlying challenges, developing a unified model proficient across the entire spectrum poses a significant challenge.

Enter the Text-to-Text Transfer Transformer (\texttt{T5}) \cite{raffel2020exploring}. This work by Raffel et al. introduces transfer learning in NLP, aiming to craft a versatile model that can be used for any NLP problem. In essence, T5 models first learn the basics of language. Then, they're sharpened for particular tasks using targeted data. It's common to find models that have been trained in this manner for any specific NLP problem.

\subsection{\texttt{doc2query} Document Expansion}\label{sec:doc2query}
Traditional retrieval techniques, such as \texttt{BM25}, rely primarily on term occurrences in both queries and documents. However, they often overlook the semantics of the content. As a result, documents that may be semantically relevant to a query might be scored as non-relevant due to differences in syntax or terminology. Dense retrieval methods, which emphasize semantic similarities between texts, can address this problem but are computationally taxing during retrieval.

A notable solution to this is the \texttt{doc\-2query} method proposed by Nogueira et al. \cite{nogueira2019document}. It employs a text-to-text transformer to convert documents into queries. By generating and appending a few of these transformed queries to the original document, classical retrieval methods show significantly improved performance. This is because these additional queries often capture semantic nuances similar to those in the actual query \cite{nogueira2019document,nogueira2019doc2query,pradeep2021expando}. Importantly, \texttt{doc\-2query} shifts the computational load to the indexing phase, ensuring minimal performance lag during retrieval. By leveraging the \texttt{T5} model, the authors further enhanced the query generation quality, leading to the variation known as \texttt{doc\-TTTTTquery}, \texttt{doc\--T5query}, or \texttt{doc\-2query\--T5} \cite{nogueira2019doc2query}.

\subsection{\texttt{SPARTA} Sparse Retrieval}\label{sec:sparta}
\texttt{SPARTA}, introduced by Zhao et al. \cite{zhao2020sparta}, is an approach in the field of sparse information retrieval. At its core, it works by encoding documents into sparse representations during the indexing phase. These representations not only capture the document's actual content but also incorporate terms that are semantically resonant, even if they're not present in the document. This underlying principle echoes the rationale of approaches like \texttt{doc2query} and dense retrieval models.

Yet, where \texttt{SPARTA} differentiates itself is in its retrieval phase. Unlike dense retrieval models, it retrieves relevant documents using straightforward index lookups, mirroring lexical retrieval strategies like \texttt{BM25} \cite{zhao2020sparta}. However, in real-world applications, \texttt{SPARTA} faces challenges: Several other models, including \texttt{BM25} and \texttt{doc2query-T5}, surpass it in ranking quality. Additionally, its generated index is substantially larger compared to alternatives like \texttt{doc2query-T5} \cite{thakur2021beir}.

\subsection{\texttt{ANCE} Dense Retrieval}\label{sec:ance}
Dense retrieval is a newer approach in information retrieval. Traditional methods, like \texttt{BM25}, rely on specific term matching in high-dimensional spaces. These methods can sometimes miss the deeper meaning or context because they focus on explicit word matches. In contrast, dense retrieval uses continuous vector spaces to embed both queries and documents. In this approach, items that have similar meanings are closer together in the vector space, even if they don't share exact terms.

\texttt{ANCE} (Approximate Nearest Neighbor Negative Contrastive Estimation) is an innovation in dense retrieval introduced by Xiong et al. \cite{xiong2020ance}. The main idea behind \texttt{ANCE} is to fix a problem in the training of dense retrieval models. Often, the irrelevant documents or "negatives", i.e. documents that should be ranked the lowest for a given query, used during training don't match the kind of irrelevant documents that come up during real-world testing. This mismatch can weaken the model's performance. To address this, \texttt{ANCE} updates its training data using an Approximate Nearest Neighbor (ANN) index. This index gets refreshed during the training process, making sure that the irrelevant documents chosen for training are similar to what the model will see in real-world scenarios. Thanks to this, \texttt{ANCE} retrieval pipelines perform better than those with other dense and sparse retrieval methods. In fact, \texttt{ANCE} achieves nearly the same accuracy as some sophisticated methods but is about 100 times faster \cite{xiong2020ance}.

\subsection{\texttt{monoT5} \& \texttt{duoT5} Rerankers}\label{sec:rerankers}
\texttt{monoT5} and \texttt{duoT5} are neural re-rankers, also developed by Nogueira et al., which attempt to inject semantic understanding into the retrieval process \cite{nogueira2020document,nogueira2019multi}. Using the \texttt{T5} model, they re-rank a list of documents based on their semantic relevance to a given query. Specifically, \texttt{monoT5} processes a query and a single document, outputting a relevance score. In contrast, \texttt{duoT5} considers a query and two documents, determining which document is more relevant. Although \texttt{duoT5} offers a more nuanced ranking, its pairwise comparison method makes it computationally heavier. Hence, a staged re-ranking approach is proposed: first using \texttt{monoT5} for the top $k$ documents and subsequently applying \texttt{duoT5} to a smaller subset, the top $l$, where $l \ll k$ \cite{nogueira2019multi,pradeep2021expando}.


\subsection{Expando-Mono-Duo Design Pattern}\label{sec:expando}
The same research team introduced a design pattern for integrating the above tools into retrieval pipelines, termed the Expando-Mono-Duo design pattern \cite{pradeep2021expando}. Here's how it works: During indexing, \texttt{doc2query-T5} is employed to enhance document representation and better the initial retrieval results from methods like \texttt{BM25}. The retrieved results are then re-ranked with \texttt{monoT5}. A selected top tier from this list undergoes another re-ranking using \texttt{duoT5}. Trials show that this composite approach leads to marked improvements in result quality across multiple evaluation metrics \cite{pradeep2021expando}.

\subsection{Conversational Query Rewriting}\label{sec:cqr}
Conversational search engines distinguish themselves from standard search engines by determining document relevance through the entirety of a conversation, not just the immediate query. In conversational contexts, subsequent questions often lean on prior interactions, implying that previous questions and answers must be factored in when fetching relevant documents. However, there's also a need to cater to conversation shifts where the immediate query doesn't relate to preceding exchanges. Blindly considering the entire conversational history in such cases could be detrimental to retrieval accuracy.

Elgohary et al. address this challenge with an innovative approach \cite{elgohary2019can}. They suggest reshaping the current query based on the overarching conversation. This reformulated query is designed to function autonomously within conventional retrieval pipelines. In essence, this technique extends the utility of standard search engines to conversational question-answering scenarios by introducing a preceding conversational query rewriting stage.

Employing text-to-text transformers, like \texttt{T5}, can be instrumental in achieving this rewrite. These models are trained to reformulate the user entered query, factoring in the conversational context. Studies validate the effectiveness of this approach, highlighting its capacity to enhance the retrieval accuracy of traditional search engines in conversational contexts \cite{elgohary2019can,anantha2020open,Lajewska:2023:ECIR}.

\subsection{\texttt{WaterlooClarke} Conversational Search Engine}\label{sec:waterlooclarke}
In the third year of the TREC Conversational Assistance Track\footnote{URL: \url{https://www.treccast.ai/}}, the standout system was \texttt{WaterlooClarke}. It used a complex retrieval pipeline, as outlined in \cite{Lajewska:2023:ECIR,yan2021waterlooclarke}. The process starts with a \texttt{T5} component that rewrites the user's query. This step transforms a context-dependent question into one that's more general, free from the specific conversation's context, as outlined in Section \ref{sec:cqr}. Following this, the rewritten query is passed into two parallel retrieval stages: firstly, sparse retrieval through \texttt{BM25}, enhanced with pseudo-relevance feedback, and, secondly, dense retrieval powered by \texttt{ANCE}. To accommodate both approaches, \texttt{WaterlooClarke} employs two indices: a lexical one for \texttt{BM25} and an ANN index for \texttt{ANCE}. Once both retrieval stages conclude, their results are fused into a single list of ranked documents. This ranked list undergoes further refinement, first with \texttt{monoT5} and then a narrower re-ranking using \texttt{duoT5} for the top-tier documents \cite{Lajewska:2023:ECIR,yan2021waterlooclarke}.



\section{Baseline Method}\label{sec:baseline}
Our baseline method is inspired by the baseline method presented by Łajewska et al. \cite{Lajewska:2023:ECIR}. It is structured in the following sequence:
\begin{enumerate}
	\item	\texttt{T5} Query Rewriting
	\item	\texttt{BM25} Retrieval
	\item	Re-ranking
			\begin{enumerate}
				\item	Re-ranking using \texttt{monoT5}
				\item	Top-document re-ranking using \texttt{duoT5}
			\end{enumerate}
\end{enumerate}

\subsection{\texttt{T5} Query Rewriting}
In conversation search engines, query rewriting is the crucial component to include the semantics of the conversation history into the currently asked query, which results into a singular rewritten query that can be fed into the retrieval pipeline.

For this purpose, we include all the previously rewritten queries $q'_0 \dots q'_{n-1}$ of our conversation, as well as the response $r_{n-1}$ of the CSE to the previous rewritten query $q'_{n-1}$ into the current query $q_n$. This is done by concatenating the previous rewritten queries and the response into a single string:
\begin{align*}
	q'_n \coloneqq q'_0 \text{<sep>} \dots \text{<sep>} q'_{n-1} \text{<sep>} r_{n-1} \text{<sep>} q_n
\end{align*}
This approach resembles the approach taken by Łajewska et al. \cite{Lajewska:2023:ECIR}.

We have found through experimentation that this mere concatenation is insufficient to produce satisfiable results: The concatenated string is too long, and too much focus during the later retrieval is being put on $r_{n-1}$, which make responses sudden topic changes impossible.

Driven by these insights, we have turned our attention to other query rewriting techniques. \texttt{Pyterrier} provides the \texttt{Sequential\-Dependence} query rewriting method\footnote{URL: \url{https://pyterrier.readthedocs.io/en/latest/rewrite.html\#sequentialdependence}}. We have found, however, that this rewriter also does not produce the desired results.

Subsequent exploration led us to the \texttt{T5} neural query rewriter trained for conversational question rewriting, see Section \ref{sec:cqr}. With this method, $q'_n$ closely mirrored $q_n$, subtly infusing it with the conversation's context, particularly when no drastic topic alterations were identified. A valuable by-product was the concise nature of the rewritten query, a departure from the growing length observed previously. Since retrieval latency is a critical factor, we utilized a smaller \texttt{T5} model: \texttt{castorini\-/t5\--base\--canard}\footnote{URL: \url{https://huggingface.co/castorini/t5-base-canard}}

\subsection{\texttt{BM25} Retrieval}
We settled on the \texttt{BM25} retrieval method, a commonly used formula in the realm of information retrieval, for its simplicity and its deployment in the reference system, allowing for direct comparisons.

\subsection{Re-ranking}
The re-ranking stage of our baseline system consists of two stages: First, the top 1000 documents retrieved by the \texttt{BM25} retrieval method are re-ranked using the \texttt{monoT5} reranker. Afterwards, the top 50 documents of the previous re-ranking stage are rearranged using the \texttt{duoT5} reranker, see Section \ref{sec:rerankers}. The precise count of documents subject to reranking at each stage is a hyperparameter of our system, allowing to balance computational cost and result quality. These rerankers were implemented in the \texttt{pyterrier\_t5} library.\footnote{URL: \url{https://github.com/terrierteam/pyterrier_t5}} Again, since a low latency of our retrieval pipeline is crucial to us, we utilized smaller \texttt{T5} models: \texttt{castorini\-/monot5\--base\--msmarco}\footnote{URL: \url{https://huggingface.co/castorini/monot5-base-msmarco}} for \texttt{monoT5} and \texttt{castorini\-/duot5\--base\--msmarco}\footnote{URL: \url{https://huggingface.co/castorini/duot5-base-msmarco}} for \texttt{duoT5}.


\section{Advanced Method}
In this paper we propose two separate extensions to the baseline model at different stages of the retrieval pipeline, making way for a total of three advanced retrieval methods. The first extension is the integration of the \texttt{RM3} query expansion method, see Section \ref{sec:baseline+rm3}. The second extension is the integration of the \texttt{doc2query} document expansion method, see Section \ref{sec:doc2query-method}. Both extensions can be combined, resulting in the final advanced retrieval method \texttt{doc2query} + \texttt{RM3}, see Section \ref{sec:doc2query-method+rm3}. In the following we will describe these individual methods in detail.

\subsection{Incorporating Pseudo-Relevance Feedback into Our Baseline}\label{sec:baseline+rm3}
Recognizing the substantial performance enhancements associated with pseudo-relevance feedback, we felt compelled to integrate a query expansion mechanism into our baseline retrieval method, see Section \ref{sec:baseline}. Our choice fell upon the \texttt{RM3} query expansion technique, well-established for its robustness and acceptance within the information retrieval community. For a deeper dive into its mechanics and principles, readers are directed to Section \ref{sec:prf}.

In the \texttt{Pyterrier} framework, the setup requires that any query expansion follows an initial retrieval phase. This initial retrieval fetches the top $p$ documents, forming the foundation for subsequent query expansion by $n$ words using \texttt{RM3}. With the query expanded, it's then passed into a secondary retrieval phase to retrieve the final document set for the end-user. And, to fine-tune the output, we again apply re-ranking using both \texttt{monoT5} and \texttt{duoT5}.

Henceforth, we'll label this integrated retrieval approach as "baseline + \texttt{RM3}", which is structured as follows:
\begin{enumerate}
	\item	\texttt{T5} Query Rewriting
	\item	\texttt{BM25} Retrieval
	\item	\texttt{RM3} Pseudo-Relevance Feedback Query Expansion
	\item	\texttt{BM25} Retrieval
	\item	Re-ranking
			\begin{enumerate}
				\item	Re-ranking using \texttt{monoT5}
				\item	Top-document re-ranking using \texttt{duoT5}
			\end{enumerate}
\end{enumerate}

\subsection{Document Expansion Method}\label{sec:doc2query-method}
To improve the results of our baseline system, we made the decision to integrate a document expansion mechanism, and after careful consideration, our choice landed on \texttt{doc2query-T5}, see Section \ref{sec:doc2query}. This approach combines the power of document expansion with the capabilities of the T5 sequence-to-sequence model to enhance the effectiveness of our information retrieval system.

The core idea behind the \texttt{doc2query-T5} model is to dynamically generate specific questions or queries that are closely related to the content of a given document. These generated questions are then seamlessly incorporated into the document. The goal of this process is to expand the document's content, thereby providing additional information that can significantly improve the effectiveness of our information retrieval system. By generating relevant queries based on the document's content, we are essentially expanding the scope of potential search terms, enabling our system to better capture the user's intent and find more relevant documents.

The integration of the \texttt{T5} model allows us to transform the document into highly relevant queries tailored to the content of the document. This is achieved by utilizing a \texttt{T5} model fine-tuned on this task of understanding the contextual relationships within the document and generate queries that effectively summarize the key points of the document. In particular, we use the \texttt{castorini\-/doc2query\--t5\--large\--msmarco}\footnote{https://huggingface.co/castorini/doc2query-t5-large-msmarco} model.

The use of \texttt{doc2query-T5} will be added to our baseline, which will otherwise remain unchanged. In particular, \texttt{doc2query-T5} can be seen as a preprocessing step to indexing, where first $m$ queries can be generated for each document in the collection, which then will be appended to the original document to form the input for the indexing stage. Therefore, this pipeline follows the Expando-Mono-Duo Design Pattern, as introduced in Section \ref{sec:expando}. The system architecture for this pipeline, which we will refer to as "\texttt{doc2query}", will therefore take the following form:
\begin{enumerate}
	\setcounter{enumi}{-1}
	\item	\texttt{doc2query-T5} Document Expansion
	\item	\texttt{T5} Query Rewriting
	\item	\texttt{BM25} Retrieval
	\item	Re-ranking
			\begin{enumerate}
				\item	Re-ranking using \texttt{monoT5}
				\item	Top-document re-ranking using \texttt{duoT5}
			\end{enumerate}
\end{enumerate}

\subsection{Extending the Document Expansion Method with Pseudo-Relevance Feedback}\label{sec:doc2query-method+rm3}
The combined "\texttt{doc2query} + \texttt{RM3}" approach represents a powerful retrieval pipeline that leverages the strengths of both document expansion and query expansion. By seamlessly integrating document expansion through \texttt{doc2query-T5} and the established pseudo-relevance feedback method \texttt{RM3}, we are able to improve our search capabilities in a number of ways.

This advanced retrieval method allows us to create more contextually relevant queries, starting with the generation of document-specific questions and refining user queries using \texttt{T5}. The subsequent search phase, guided by \texttt{BM25}, reduces the number of candidate documents. \texttt{RM3} then uses these candidates to create additional queries, thereby broadening the search field. In a final round of searching using \texttt{BM25}, we broaden the set of documents. To further improve the quality of the results, our \texttt{monoT5} and \texttt{duoT5} re-ranking steps ensure that the most relevant documents come out on top. This approach offers a holistic solution that not only improves accuracy but also explores a wider range of potentially relevant documents, providing users with an improved and efficient information search experience. Ultimately, our architecture is a combination of \texttt{RM3} and \texttt{doc2query-T5}, see Section \ref{sec:related}, and will take the following form:
\begin{enumerate}
	\setcounter{enumi}{-1}
	\item	\texttt{doc2query-T5} Document Expansion
	\item	\texttt{T5} Query Rewriting
	\item	\texttt{BM25} Retrieval
	\item	\texttt{RM3} Pseudo-Relevance Feedback Query Expansion
	\item	\texttt{BM25} Retrieval
	\item	Re-ranking
			\begin{enumerate}
				\item	Re-ranking using \texttt{monoT5}
				\item	Top-document re-ranking using \texttt{duoT5}
			\end{enumerate}
\end{enumerate}

\section{Results}\label{sec:results}
The individual methods were evaluated on the MS MARCO document collection and the following provided files: \texttt{queries\-\_\-train\-.csv}, comprising of a list of queries grouped together into several conversation sessions, and \texttt{qrels\-\_\-train\-.txt} that contains the relevance assessments for the training queries. Our evaluation focused on a suite of metrics:
\begin{itemize}
	\item	Recall at 1000 (R@1000)
	\item	Mean Average Precision (MAP)
	\item	Mean Reciprocal Rank (MRR)
	\item	Normalized Discounted Cumulative Gain (NDCG\_Cut@3)
\end{itemize}
These metrics were calculated using the \texttt{trec\_eval} tool:
\begin{verbatim}
trec_eval -c -m recall.1000 -m map -m recip_rank
-m ndcg_cut.3 -l2 -M1000 qrels_train.txt
{GENERATED_TREC_RUNFILE}
\end{verbatim}

As stated in Section \ref{sec:baseline}, the baseline method can be parameterized in a few different ways. For this evaluation, we utilized the following configurations: The document retrieval (\texttt{BM25}) used the default parameters from \texttt{pyterrier}\footnote{URL: \url{https://pyterrier.readthedocs.io/en/latest/terrier-retrieval.html}} to retrieve the 1000 most-relevant documents for each query. All 1000 documents were then reranked using the \texttt{monoT5} reranker. Because of the high computational cost of the \texttt{duoT5} reranker, only of those 1000 documents the best 50 documents were then reordered using this reranker.

For the first extension of the baseline, the baseline + \texttt{RM3} method, we utilized the same configuration for these components. The \texttt{RM3} query expansion component was parameterized to expand the query by 26 terms, using the top 17 documents retrieved by the initial \texttt{BM25} search.

The \texttt{doc2query} and \texttt{doc2query} + \texttt{RM3} strategies built upon the baseline and baseline + \texttt{RM3} designs. The distinction is in the initial indexing stage (refer to Sections \ref{sec:doc2query-method} and \ref{sec:doc2query-method+rm3}). Keeping everything else constant for a fair comparison, the indexing stage was adjusted to generate 3 descriptive queries for each document. The subsequent documents incorporated these queries before indexing.

We ran our evaluations on a Google Cloud Compute Instance, powered by a single Nvidia L4 Tensor Core GPU. All retrieval pipelines exhibited similar retrieval times.

\begin{table}[h]
\begin{center}
	\caption{Performance of the different methods on the MS MARCO document collection. Best results are in bold, second-best results are underlined.}
	\begin{tabular}{l|rrrr}
			& MAP & MRR & R@1000 & NDCG\_Cut@3 \\
		\hline
		Reference System & 0.07 &  &  & 0.09 \\
		Baseline & $0.1814$ & $0.3062$ & $0.4945$ & $0.2310$ \\
		Baseline + \texttt{RM3} & $0.1874$ & $0.3158$ & $\dashuline{0.5054}$ & $\mathbf{0.2518}$ \\
		\texttt{doc2query} & $\mathbf{0.1971}$ & $\mathbf{0.3281}$ & $\mathbf{0.5393}$ & $\dashuline{0.2517}$ \\
		\texttt{doc2query} + \texttt{RM3} & $\dashuline{0.1878}$ & $\dashuline{0.3217}$ & $0.4996$ & $0.2421$
	\end{tabular}
	\label{table:1}
\end{center}
\end{table}

As can be seen in table \ref{table:1}, our baseline method is at least as efficient as the reference system provided by the project owners. Thus, our baseline method fulfills the hard requirement of the project.

Furthermore, comparing the results of the baseline with those of the advanced methods, we can see that the introduction of \texttt{RM3} and \texttt{doc2query} significantly improves the retrieval quality. In particular, the \texttt{doc2query} method achieves the best results in almost all four of the metrics, while the baseline + \texttt{RM3} method just barely surpasses the \texttt{doc2query} approach in the NDCG\_Cut@3 metric.

One might expect the \texttt{doc2query} + \texttt{RM3} combination to significantly improve upon the other methods. Yet, the results show that it is more similar to the baseline + \texttt{RM3} method performance-wise, following the superior \texttt{doc2query} approach by quite a margin.

Finally, a glance at index generation: Indexing the 8.8 million documents of MS MARCO was a breeze for the baseline methods, finishing in under an hour. However, \texttt{doc2query} required about six continuous days to generate all descriptive queries. After this, indexing was once again swift, under an hour. Both indices store the original document content as required for query rewriting, see Section \ref{sec:cqr}. The baseline approaches generated a $4.2$GB index, whereas the additional terms produced by \texttt{doc2query} expanded it slightly to $4.5$GB. In theory, retrieving from the bigger index should be slower, but we didn't observe significant differences.

\section{Discussion and Conclusions}
This study introduced and analyzed four conversational retrieval strategies for the MS MARCO document collection. Drawing inspiration from the baseline approach by Łajewska et al. \cite{Lajewska:2023:ECIR}, our baseline method demonstrated comparable, if not superior, efficiency to the reference system presented by the project curators.

Our findings further highlight the advantages of extending the baseline approach. Incorporating either a query expansion or a document expansion mechanism significantly enhances the quality of document rankings across all evaluated metrics. Interestingly, combining both the query and document expansion elements didn't consistently yield expected improvements. One plausible explanation centers on our initial sparse retrieval stage, that is \texttt{BM25}. From a semantic viewpoint, this stage may be insufficient in recognizing relevant documents that possess syntactic deviations from the query. Hence, even with the aid from \texttt{doc2query} and \texttt{RM3}, there's potential room for missing relevant documents.

If our theory holds, introducing a dense retrieval technique, like \texttt{ANCE}, see Section \ref{sec:ance}, might bridge this semantic gap, refining the precision of document rankings. Additionally, it's worth exploring the reason behind the performance dip of the \texttt{doc2query} + \texttt{RM3} strategy in the mean reciprocal rank metric, especially when compared against the results from the baseline + \texttt{RM3} and sole \texttt{doc2query} methods.

In conclusion, while infusing both query and document expansion strategies substantially boosts retrieval quality, the benefits aren't without implications. The bright side is that these enhancements introduce insignificant extra computational costs during retrieval, showcasing a practical approach for improving conversational retrieval systems.


%%
%% If your work has an appendix, this is the place to put it.
%\appendix

%\section{Appendix}


%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}

\newpage
\appendix
\section{Division of Work During the Project}
Concerning the distribution of tasks on the project, we worked in a fairly classic way, trying to be as organized as possible to make sure we did things well. In general, we tried to work together on each part and then split the work when necessary.

For part G1, we both worked together on the baseline, thinking together about the architecture and how to code.

For the G2 part, given that it was purely research work, we divided it up in order to move forward correctly. At the same time, we constantly exchanged information on our research and what could be interesting to discuss in our report.

For part G3, we worked together on the advanced methods that we discussed in G2. We took each method one by one and implemented it in such a way that we could understand the code together and evaluate its effectiveness.
\end{document}
\endinput
